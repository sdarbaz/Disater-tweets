
import pandas as pd

# Load the dataset
file_path = '/content/disaster_tweets_data(DS).csv'
data = pd.read_csv(file_path)
# Display the first few rows to understand the data structure
print(data.head())

# Check for null values
print(data.isnull().sum())

# Drop rows with null values (if any)
data = data.dropna()

import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
import re

# Download necessary NLTK data
nltk.download('punkt')
nltk.download('stopwords')

# Function to preprocess tweets
def preprocess_tweet(tweet):
    # Convert to lower case
    tweet = tweet.lower()
    # Remove punctuations
    tweet = re.sub(r'[^\w\s]', '', tweet)
    # Tokenize
    words = nltk.word_tokenize(tweet)
    # Remove stop words
    stop_words = set(stopwords.words('english'))
    words = [word for word in words if word not in stop_words]
    # Apply stemming
    stemmer = PorterStemmer()
    words = [stemmer.stem(word) for word in words]
    return ' '.join(words)

# Apply preprocessing
data['tweets'] = data['tweets'].apply(preprocess_tweet)

from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer

# Choose one of the vectorizers
vectorizer = CountVectorizer()  # or TfidfVectorizer()

# Fit and transform the tweets
X = vectorizer.fit_transform(data['tweets'])

y = data['target']

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

rom sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import confusion_matrix, classification_report

# Define models
models = {
    "Multinomial Naive Bayes": MultinomialNB(),
    "Logistic Regression": LogisticRegression(),
    "KNN Classification": KNeighborsClassifier()
}

# Function to apply models
def apply_models(models, X_train, y_train, X_test):
    for name, model in models.items():
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        print(f"Results for {name}:\n")
        print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
        print("\nClassification Report:\n", classification_report(y_test, y_pred))
        print("-" * 50)

# Apply models
apply_models(models, X_train, y_train, X_test)

import matplotlib.pyplot as plt
import seaborn as sns

# Function to plot confusion matrix
def plot_confusion_matrix(cm, model_name):
    plt.figure(figsize=(6, 5))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
    plt.title(f"Confusion Matrix for {model_name}")
    plt.xlabel("Predicted Label")
    plt.ylabel("True Label")
    plt.show()

# Updated function to apply models and plot confusion matrix
def apply_models_and_plot(models, X_train, y_train, X_test, y_test):
    model_performance = {}
    for name, model in models.items():
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        cm = confusion_matrix(y_test, y_pred)
        print(f"Results for {name}:\n")
        print("Confusion Matrix:\n", cm)
        print("\nClassification Report:\n", classification_report(y_test, y_pred))
        plot_confusion_matrix(cm, name)
        accuracy = (cm[0,0] + cm[1,1]) / cm.sum()
        model_performance[name] = accuracy
        print("-" * 50)
    return model_performance

# Apply models and plot confusion matrix
model_performance = apply_models_and_plot(models, X_train, y_train, X_test, y_test)

# Finding the model with the best accuracy
best_model = max(model_performance, key=model_performance.get)
print(f"The model with the best accuracy is: {best_model} with an accuracy of {model_performance[best_model]:.2f}")

# Step 10: Report the model with the best accuracy
# Calculate the accuracy of each model
accuracy_nb = models["Multinomial Naive Bayes"].score(X_test, y_test)
accuracy_lr = models["Logistic Regression"].score(X_test, y_test)
accuracy_knn = models["KNN Classification"].score(X_test, y_test)

# Print the accuracies
print(f"\nAccuracy for Multinomial Naïve Bayes: {accuracy_nb}")
print(f"Accuracy for Logistic Regression: {accuracy_lr}")
print(f"Accuracy for KNN Classification: {accuracy_knn}")

# Determine the best model
best_model = max(accuracy_nb, accuracy_lr, accuracy_knn)
if best_model == accuracy_nb:
    print("The best model is Multinomial Naïve Bayes.")
elif best_model == accuracy_lr:
    print("The best model is Logistic Regression.")
else:
    print("The best model is KNN Classification.")

odels to obtain the accuracy scores
# Since the models have already been trained in previous steps, we will just calculate the scores again.

accuracy_nb = models["Multinomial Naive Bayes"].score(X_test, y_test)
accuracy_lr = models["Logistic Regression"].score(X_test, y_test)
accuracy_knn = models["KNN Classification"].score(X_test, y_test)

# Print the accuracies
print(f"Accuracy for Multinomial Naïve Bayes: {accuracy_nb}")
print(f"Accuracy for Logistic Regression: {accuracy_lr}")
print(f"Accuracy for KNN Classification: {accuracy_knn}")

# Find the best model
accuracies = {
    "Multinomial Naive Bayes": accuracy_nb,
    "Logistic Regression": accuracy_lr,
    "KNN Classification": accuracy_knn
}
best_model_name = max(accuracies, key=accuracies.get)
best_model = models[best_model_name]
best_model_accuracy = accuracies[best_model_name]

print(f"The best model is {best_model_name} with an accuracy of {best_model_accuracy:.4f}")

# Predict sentiments using the best model
predicted_sentiments = best_model.predict(X)

# Add the predictions to the dataframe
data['PredictedSentiment'] = predicted_sentiments

# Assuming 0 is negative and 1 is positive
data['SentimentAnalysis'] = data['PredictedSentiment'].map({0: 'Negative', 1: 'Positive'})

# Plot the sentiment distribution
sentiment_counts = data['SentimentAnalysis'].value_counts()
plt.figure(figsize=(8, 6))
sns.barplot(x=sentiment_counts.index, y=sentiment_counts.values, palette='coolwarm')
plt.title('Predicted Sentiment Analysis')
plt.xlabel('Sentiment')
plt.ylabel('Counts')
plt.show()

# Pie chart for sentiment distribution
plt.figure(figsize=(8, 8))
data['SentimentAnalysis'].value_counts().plot.pie(autopct='%1.1f%%', startangle=140, colors=['red', 'green'])
plt.title('Pie Chart of Sentiment Distribution')
plt.ylabel('')  # Hide the y-label
plt.show()

from wordcloud import WordCloud

# Generate a word cloud for positive sentiments
positive_tweets = data[data['SentimentAnalysis'] == 'Positive']['tweets']
positive_text = " ".join(tweet for tweet in positive_tweets)

wordcloud = WordCloud(background_color="white").generate(positive_text)

# Display the generated image
plt.figure(figsize=(10, 7))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.title('Word Cloud for Positive Sentiments')
plt.show()
